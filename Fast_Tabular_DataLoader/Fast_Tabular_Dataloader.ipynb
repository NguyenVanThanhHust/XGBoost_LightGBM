{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTensorDataLoader:\n",
    "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
    "        \"\"\"\n",
    "        *tensor: tensors to store. Must have same length dim0\n",
    "        batch_size: batch size to load\n",
    "        shuffle: if True, shuffle  \n",
    "        return FastTEnsorDataLoader\n",
    "        \"\"\"\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Calculate num batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder>0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            r = torch.randperm(self.dataset_len)\n",
    "            sefl.tensors = [t[r] for t in self.tensors]\n",
    "        self.i = 0\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        batch = tuple(t[self.i:self.i + self.batch_size] for t in self.tensors)\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os \n",
    "import os.path as osp\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data params\n",
    "ROW_LIMIT = None # for quicker testing\n",
    "NUM_TEST_ROWS = 500000\n",
    "LABEL_COLUMN = 0\n",
    "FEATURE_COLUMNS = list(range(1, 22)) # low-level features only as per http://archive.ics.uci.edu/ml/datasets/HIGGS\n",
    "FILE_NAME = 'HIGGS.csv'\n",
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not osp.isfile(FILE_NAME):\n",
    "    print(\"check file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "BATCH_SIZE = 16384\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name:str, test_rows:int, feature_columns:List[int], label_column:int, row_limit:int):\n",
    "    data = pd.read_csv(file_name, header=None, dtype='float32', nrows=row_limit)\n",
    "    \n",
    "    features = torch.from_numpy(data.loc[:, feature_columns].reset_index(drop=True).values)\n",
    "    labels = torch.from_numpy(data.loc[:, label_column].reset_index(drop=True).values)\n",
    "    \n",
    "    train_x = features[:-test_rows]\n",
    "    train_y = labels[:-test_rows]\n",
    "    \n",
    "    test_x = features[-test_rows:]\n",
    "    test_y = labels[-test_rows:]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_x, train_y, test_x, test_y = load_data(file_name=FILE_NAME, test_rows=NUM_TEST_ROWS,\n",
    "            feature_columns=FEATURE_COLUMNS, label_column=LABEL_COLUMN, row_limit=ROW_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(gpu:bool=True):\n",
    "    dropout_rate=0.5\n",
    "    hidden_units=300\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(len(FEATURE_COLUMNS), hidden_units),\n",
    "        nn.Tanh(),\n",
    "        nn.Dropout(p=dropout_rate), \n",
    "        nn.Linear(hidden_units, hidden_units),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_units, hidden_units),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_units, hidden_units),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_units, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    if gpu:\n",
    "        model = model.cuda()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module, train_dataloader:DataLoader,\n",
    "          test_x:torch.Tensor, test_y:torch.Tensor, \n",
    "         epochs: int, name: str, log_every_n_steps=100, \n",
    "          eval_every_n_steps=100, gpu: bool=True):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters())\n",
    "    loss_fn = nn.BCELoss(reduction='mean')\n",
    "    \n",
    "    # log to tensorboard\n",
    "    writer = SummaryWriter(comment=name)\n",
    "    \n",
    "    global_step = 0\n",
    "    if gpu:\n",
    "        test_x = test_x.cuda()\n",
    "        test_y = test_y.cuda()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            if gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "            y_pred = model(x_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred.squeeze(), y_batch)\n",
    "            \n",
    "            if global_step % log_every_n_steps == log_every_n_steps - 1:\n",
    "                writer.add_scalar('Loss train', loss, global_step)\n",
    "                writer.add_scalar('Epoch', epoch, global_step)\n",
    "                roc_auc = roc_auc_score(y_batch.cpu().numpy(), y_pred.cpu().detach().numpy())\n",
    "                writer.add_scalar('ROC_AUC/train', roc_auc, global_step)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if global_step % eval_every_n_steps == eval_every_n_steps - 1:\n",
    "                test_y_pred = model(test_x)\n",
    "                test_loss = loss_fn(test_y_pred.squeeze(), test_y)\n",
    "                writer.add_scalar('Loss/test', test_loss, global_step)\n",
    "                test_roc_auc = roc_auc_score(test_y.cpu().numpy(), test_y_pred.cpu().detach().numpy())\n",
    "                writer.add_scalar('ROC_AUC/test', test_roc_auc, global_step)\n",
    "\n",
    "            global_step += 1\n",
    "        print(f'Epoch {epoch} done. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = TensorDataset(train_x, train_y)\n",
    "default_train_batches = DataLoader(data_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "fast_train_batches = FastTensorDataLoader(train_x, train_y, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# standard dataloader benchmark\n",
    "model = create_model(gpu=GPU)\n",
    "start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done. \n",
      "Epoch 1 done. \n",
      "Epoch 2 done. \n",
      "Epoch 3 done. \n",
      "Epoch 4 done. \n"
     ]
    }
   ],
   "source": [
    "train(model=model, train_dataloader=default_train_batches, test_x=test_x, test_y=test_y, epochs=NUM_EPOCHS, name='default_data_loader', gpu=GPU)\n",
    "\n",
    "default_elapsed_seconds = time.perf_counter() - start\n",
    "\n",
    "# improved dataloader benchmark\n",
    "model = create_model(gpu=GPU)\n",
    "start = time.perf_counter()\n",
    "\n",
    "train_for_n_epochs(model=model, train_dataloader=fast_train_batches, test_x=test_x, test_y=test_y, epochs=NUM_EPOCHS, name='custom_data_loader', gpu=GPU)\n",
    "\n",
    "fast_elapsed_seconds = time.perf_counter() - start\n",
    "\n",
    "print(f'Standard dataloader: {default_elapsed_seconds/NUM_EPOCHS:.2f}s/epoch.')\n",
    "print(f'Custom dataloader: {fast_elapsed_seconds/NUM_EPOCHS:.2f}s/epoch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
